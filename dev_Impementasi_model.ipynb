{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cbYMstnTHI9"
      },
      "source": [
        "#**Operasi File.csv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-qiirHyHI7Wk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import pymysql"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_mysql_table(table, host='localhost', user='root', password='rootku', database='review'):\n",
        "    # Establish a connection to the MySQL database\n",
        "    connection = pymysql.connect(\n",
        "        host=host,\n",
        "        user=user,\n",
        "        password=password,\n",
        "        database=database\n",
        "    )\n",
        "    \n",
        "    # Create a cursor object to execute SQL queries\n",
        "    cursor = connection.cursor()\n",
        "    \n",
        "    query = f\"SELECT * FROM {table}\"\n",
        "    cursor.execute(query)\n",
        "    result = cursor.fetchall()\n",
        "    \n",
        "    # Convert the result to a Pandas DataFrame\n",
        "    df = pd.DataFrame(result)\n",
        "    \n",
        "    # Assign column names based on the cursor description\n",
        "    df.columns = [column[0] for column in cursor.description]\n",
        "    \n",
        "    # Close the cursor and the database connection\n",
        "    cursor.close()\n",
        "    connection.close()\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_review</th>\n",
              "      <th>nama</th>\n",
              "      <th>tanggal</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>29</td>\n",
              "      <td>aku</td>\n",
              "      <td>2023-12-06</td>\n",
              "      <td>bagus sekali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30</td>\n",
              "      <td>aku</td>\n",
              "      <td>2023-12-06</td>\n",
              "      <td>bagus sekali</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id_review nama     tanggal        review\n",
              "0         29  aku  2023-12-06  bagus sekali\n",
              "1         30  aku  2023-12-06  bagus sekali"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "table_name = 'input_review'\n",
        "df = read_mysql_table(table_name)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    bagus sekali\n",
              "1    bagus sekali\n",
              "Name: review, dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#menyimpan tweet. (tipe data series pandas)\n",
        "data_reviews = df['review']\n",
        "data_reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0Cs0wkoUiUc",
        "outputId": "ced8165c-1371-450b-e0c4-cded20e211ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    bagus sekali\n",
              "1    bagus sekali\n",
              "Name: review, dtype: object"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_casefolding = data_reviews.str.lower()\n",
        "data_casefolding.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw9RfRtSVjKD"
      },
      "source": [
        "##Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbue26Isbfqi"
      },
      "source": [
        "Proses dalam menghilangkan karakter-karakter illegal.\n",
        "\n",
        "Contoh : %, &, >, (,\n",
        "{, ], 1-9, @uluwatu, http://..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU8wxNduVZ5M",
        "outputId": "6b55bf21-9f65-423c-e9f1-3b4b12fd68f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    bagus sekali\n",
            "1    bagus sekali\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "#filtering url -> menggunakan list comprehension dan menggunakan regular expression re.sub mengganti url dengan spasi\n",
        "filtering_url = [re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", ulasan) for ulasan in data_casefolding]\n",
        "\n",
        "#filtering cont -> menggunakan list comprehension, kata cont(continued) dianggap sebagai noise dalam proses analisis data\n",
        "filtering_cont = [re.sub(r'\\(cont\\)',\" \", Reviews)for Reviews in filtering_url]\n",
        "\n",
        "#filtering punctuatuion -> menggunakan list comprehension untuk menghapus tanda baca dari setiap ulasan\n",
        "filtering_punctuation = [re.sub('[!\"”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]', ' ', Reviews) for Reviews in filtering_cont]  #hapus simbol'[!#?,.:\";@()-_/\\']'\n",
        "\n",
        "#filtering tagger -> menghapus #tagger dan mengganti tagar dengan spasi\n",
        "filtering_tagger = [re.sub(r'#([^\\s]+)', '', Reviews) for Reviews in filtering_punctuation]\n",
        "\n",
        "#filtering numeric -> menghapus angka dari setiap ulasan dalam dataset dengan spasi\n",
        "filtering_numeric = [re.sub(r'\\d+', ' ', Reviews) for Reviews in filtering_tagger]\n",
        "\n",
        "data_filtering = pd.Series(filtering_numeric) #pandas series\n",
        "\n",
        "print (data_filtering[:4])\n",
        "#print (data_filtering) #jika ingin mencetak semua data_filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56cB9tbNWd5q",
        "outputId": "727e0c04-ea06-4601-ad96-10a5febf597a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['bagus', 'sekali'], ['bagus', 'sekali']]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/avrians/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt') #model tokenisasi kata\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#tokenisasi kata pada setiap baris\n",
        "data_tokens = [word_tokenize(line) for line in data_filtering]\n",
        "print(data_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kui9ImKuWnb5",
        "outputId": "cb12db97-b51c-44fe-bce3-d09605dbd4a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sastrawi in /home/avrians/.local/lib/python3.10/site-packages (1.0.1)\n",
            "\u001b[33mDEPRECATION: distro-info 1.1build1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.43ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o1rozCl2W_Gb"
      },
      "outputs": [],
      "source": [
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vQJuAkOXE8N",
        "outputId": "63b82faa-deba-4c03-e21c-34f83c6059ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['bagus', 'sekali'], ['bagus', 'sekali']]\n"
          ]
        }
      ],
      "source": [
        "factory = StopWordRemoverFactory()\n",
        "ind_stopword = factory.get_stop_words()\n",
        "\n",
        "#penedefinisian fungsi stopword\n",
        "def stopword(line):\n",
        "  temp = list()\n",
        "  for word in line:\n",
        "    if(len(word)>3):\n",
        "      temp.append(word)\n",
        "  return temp\n",
        "\n",
        "data_katapenting = [stopword (line) for line in data_tokens]\n",
        "print(data_katapenting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_h57VTUXRpJ",
        "outputId": "e8e85c29-757e-4998-b947-4a41be5cb57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['bagus', 'sekali'], ['bagus', 'sekali']]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#slang word\n",
        "path_dataslang = open(\"Data/kamus kata baku-clear.csv\")\n",
        "dataslang = pd.read_csv(path_dataslang, encoding = 'utf-8',header=None, sep=\";\")\n",
        "\n",
        "def replaceSlang(word):\n",
        "  if word in list(dataslang[0]):\n",
        "    indexslang = list(dataslang[0]).index(word)\n",
        "    return dataslang[1][indexslang]\n",
        "  else:\n",
        "    return word\n",
        "\n",
        "reviews_formal = []\n",
        "for data in data_tokens:\n",
        "  data_clean = [replaceSlang(word) for word in data]\n",
        "  reviews_formal.append(data_clean)\n",
        "len_reviews_formal = len(reviews_formal)\n",
        "print(reviews_formal)\n",
        "len_reviews_formal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmUvXHYfM3-5",
        "outputId": "f7d79e97-8d24-49ca-f6e1-1df8bf0d5bcf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/avrians/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "default_stop_words = nltk.corpus.stopwords.words('indonesian')\n",
        "stopwords = set(default_stop_words)\n",
        "\n",
        "def removeStopWords(line, stopwords):\n",
        "  words = []\n",
        "  for word in line:\n",
        "    word=str(word)\n",
        "    word = word.strip()\n",
        "    if word not in stopwords and word != \"\" and word != \"&\":\n",
        "      words.append(word)\n",
        "\n",
        "  return words\n",
        "data_notstopword = [removeStopWords(line,stopwords) for line in reviews_formal]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Haum7regYVvZ"
      },
      "source": [
        "##Steamming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwnRBe6Afozt"
      },
      "source": [
        "Proses stemming dilakukan dengan menghapus imbuhan, baik awalan maupun akhiran dari suatu kata untuk mendapatkan kata dasarnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0g5J2FXoYZSv"
      },
      "outputs": [],
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LCRj2-fYdRq",
        "outputId": "d3d2653d-6abe-4135-bdcc-9b445e84a6bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['bagus', 'sekali'], ['bagus', 'sekali']]\n"
          ]
        }
      ],
      "source": [
        "factory = StemmerFactory()\n",
        "ind_stemmer =factory.create_stemmer()\n",
        "\n",
        "def stemmer(line):\n",
        "  temp = list()\n",
        "  for word in line:\n",
        "      word = ind_stemmer.stem(word)\n",
        "      temp.append(word)\n",
        "  return temp\n",
        "\n",
        "reviews_dasar = [stemmer (line) for line in reviews_formal]\n",
        "\n",
        "#cetak 2 baris pertama\n",
        "print(reviews_dasar[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPHWTE7lMFec",
        "outputId": "8746c8fc-048c-4503-aabc-024edd8353b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/avrians/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /home/avrians/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/avrians/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "UbQzzO_WRLM5",
        "outputId": "83723547-de50-45f6-9c4c-6066957da57d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'bagus sekali'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['review'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "hWUnt_WiSS15",
        "outputId": "a095e65e-c4be-4385-d9aa-24154f4de947"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_review</th>\n",
              "      <th>nama</th>\n",
              "      <th>tanggal</th>\n",
              "      <th>review</th>\n",
              "      <th>CleanReview</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>29</td>\n",
              "      <td>aku</td>\n",
              "      <td>2023-12-06</td>\n",
              "      <td>bagus sekali</td>\n",
              "      <td>bagus sekali</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>30</td>\n",
              "      <td>aku</td>\n",
              "      <td>2023-12-06</td>\n",
              "      <td>bagus sekali</td>\n",
              "      <td>bagus sekali</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id_review nama     tanggal        review   CleanReview\n",
              "0         29  aku  2023-12-06  bagus sekali  bagus sekali\n",
              "1         30  aku  2023-12-06  bagus sekali  bagus sekali"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#pengembalian kata ke bentuk dasar\n",
        "lemma = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def CleanReview(txt):\n",
        "  txt = re.sub(r'http\\S+', ' ', txt) #mengganti semua url dengan spasi\n",
        "  txt = re.sub(r'[^a-zA-Z]', ' ', txt) #mengganti karakter non-alfabet dengan spasi\n",
        "  txt = str(txt).lower() #mengubah huruf besar ke kecil\n",
        "  txt = word_tokenize(txt) #teks menjadi token\n",
        "  txt = [item for item in txt if item not in data_notstopword] #menghapus kata yg ga dipake (stopword)\n",
        "  txt = [lemma.lemmatize(word=w, pos='v')for w in txt] #mengembalikan kata ke bentuk dasar\n",
        "  txt = [i for i in txt if len(i) > 2] #menghapus kata-kata yang terlalu pendek\n",
        "  txt = ' '.join(txt)\n",
        "  return txt\n",
        "df['CleanReview'] = df['review'].apply(CleanReview)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specify the file path of the pickle file\n",
        "file_path = 'Model/Model_SVM.pkl'\n",
        "\n",
        "# Read the pickle file\n",
        "with open(file_path, 'rb') as file:\n",
        "    data_train = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sebenernya ini aplikasi biasa saja hanya saja saya mendownload aplikasi ini karena diwajibkan oleh sekolah dan masalahnya aplikasi ini mengambil data pribadi yang tentu saja saya keberatan kalau ingin membantu seseorang ayolah membantu dengan ikhlas dan jagan malah mengambil data pribadi seseorang'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pembuatan vector kata\n",
        "vectorizer = TfidfVectorizer()\n",
        "train_vector = vectorizer.fit_transform(data_train)\n",
        "reviews2 = [\" \".join(r) for r in reviews_dasar]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contoh menyimpan model dan vectorizer dalam satu file pickle\n",
        "model_data = {\n",
        "    'model': svc_model,          # Gantilah dengan model SVM yang sesuai\n",
        "    'vectorizer': vectorizer     # Gantilah dengan objek vectorizer yang sesuai\n",
        "}\n",
        "\n",
        "# Menyimpan model dan vectorizer dalam satu file pickle\n",
        "with open('Model/tfidf_review.pkl', 'wb') as file:\n",
        "    pickle.dump(model_data, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "X has 3309 features, but SVC is expecting 4 features as input.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/home/avrians/Documents/Kuliah/sistem cerdas/Code/week10/mengajar-streamlit-main/dev_Impementasi_model.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/avrians/Documents/Kuliah/sistem%20cerdas/Code/week10/mengajar-streamlit-main/dev_Impementasi_model.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m test_vector \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mtransform(reviews2)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/avrians/Documents/Kuliah/sistem%20cerdas/Code/week10/mengajar-streamlit-main/dev_Impementasi_model.ipynb#X61sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Prediksi dengan model SVM\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/avrians/Documents/Kuliah/sistem%20cerdas/Code/week10/mengajar-streamlit-main/dev_Impementasi_model.ipynb#X61sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m result \u001b[39m=\u001b[39m load_model\u001b[39m.\u001b[39;49mpredict(test_vector)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/avrians/Documents/Kuliah/sistem%20cerdas/Code/week10/mengajar-streamlit-main/dev_Impementasi_model.ipynb#X61sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Sekarang 'result' berisi hasil prediksi untuk setiap data uji\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/avrians/Documents/Kuliah/sistem%20cerdas/Code/week10/mengajar-streamlit-main/dev_Impementasi_model.ipynb#X61sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:818\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    816\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_function(X), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    817\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 818\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[1;32m    819\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp))\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:431\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    416\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \n\u001b[1;32m    418\u001b[0m \u001b[39m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39m        The predicted values.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_for_predict(X)\n\u001b[1;32m    432\u001b[0m     predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_predict \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dense_predict\n\u001b[1;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m predict(X)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/svm/_base.py:611\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    608\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    610\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel):\n\u001b[0;32m--> 611\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    612\u001b[0m         X,\n\u001b[1;32m    613\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    614\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[1;32m    615\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    616\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    617\u001b[0m         reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    618\u001b[0m     )\n\u001b[1;32m    620\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sp\u001b[39m.\u001b[39missparse(X):\n\u001b[1;32m    621\u001b[0m     X \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mcsr_matrix(X)\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 626\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    628\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 415\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    416\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    418\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: X has 3309 features, but SVC is expecting 4 features as input."
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Membaca model dan vectorizer dari file pickle\n",
        "with open('Model/tfidf_review.pkl', 'rb') as file:\n",
        "    loaded_data = pickle.load(file)\n",
        "    load_model = loaded_data['model']\n",
        "    vectorizer = loaded_data['vectorizer']\n",
        "\n",
        "# Misalnya, reviews2 adalah list dari multi-line string\n",
        "# pastikan untuk menangani setiap string dengan benar (misalnya, dengan strip())\n",
        "reviews2 = [review.strip() for review in reviews2]\n",
        "\n",
        "# Pastikan bahwa reviews2 telah diproses menggunakan vectorizer yang sama\n",
        "test_vector = vectorizer.transform(reviews2)\n",
        "\n",
        "# Prediksi dengan model SVM\n",
        "result = load_model.predict(test_vector)\n",
        "\n",
        "# Sekarang 'result' berisi hasil prediksi untuk setiap data uji\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.utils.multiclass import unique_labels\n",
        "unique_labels(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['label'] = result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def delete_all_data_from_table(table, host='localhost', user='root', password='rootku', database='review'):\n",
        "    # Establish a connection to the MySQL database\n",
        "    connection = pymysql.connect(\n",
        "        host=host,\n",
        "        user=user,\n",
        "        password=password,\n",
        "        database=database\n",
        "    )\n",
        "    \n",
        "    # Create a cursor object to execute SQL queries\n",
        "    cursor = connection.cursor()\n",
        "    \n",
        "    # Delete all data from the specified table\n",
        "    query = f\"DELETE FROM {table}\"\n",
        "    cursor.execute(query)\n",
        "    \n",
        "    # Commit the changes\n",
        "    connection.commit()\n",
        "    \n",
        "    # Close the cursor and the database connection\n",
        "    cursor.close()\n",
        "    connection.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "delete_all_data_from_table('input_review')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def insert_df_into_hasil_model(df, host='localhost', user='root', password='', database='review'):\n",
        "    # Establish a connection to the MySQL database\n",
        "    connection = pymysql.connect(\n",
        "        host=host,\n",
        "        user=user,\n",
        "        password=password,\n",
        "        database=database\n",
        "    )\n",
        "\n",
        "    # Create a cursor object to execute SQL queries\n",
        "    cursor = connection.cursor()\n",
        "\n",
        "    # Insert each row from the DataFrame into the 'hasil_model' table\n",
        "    for index, row in df.iterrows():\n",
        "        query = \"INSERT INTO hasil_model (id_review, nama, tanggal, review, label) VALUES (%s, %s, %s, %s, %s)\"\n",
        "        cursor.execute(query, (row['id_review'], row['nama'], row['tanggal'], row['review'], row['label']))\n",
        "\n",
        "    # Commit the changes\n",
        "    connection.commit()\n",
        "\n",
        "    # Close the cursor and the database connection\n",
        "    cursor.close()\n",
        "    connection.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "insert_df_into_hasil_model(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "table_name = 'hasil_model'\n",
        "hasil_df = read_mysql_table(table_name)\n",
        "hasil_df.to_csv('Data/hasil_model.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
